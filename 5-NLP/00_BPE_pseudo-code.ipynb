{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef85659e-0bfb-4611-a3c8-fee13b98dae4",
   "metadata": {},
   "source": [
    "# Pseudo-Code on the Byte-Pair Encoding algorithm\n",
    "- Reference Article: [Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162/)\n",
    "- Code Generated with **Claude 3.7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d74cecd-b7e3-4cb7-b784-8b79289faec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs are friends\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d9abd9-582e-47cb-8f57-c5769e3e004a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 4, 'sat': 2, 'on': 2, 'cat': 1, 'mat': 1, 'dog': 1, 'log': 1, 'cats': 1, 'and': 1, 'dogs': 1, 'are': 1, 'friends': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "# Count the frequency of each word in the corpus\n",
    "def get_word_frequencies(corpus):\n",
    "    word_freqs = Counter()\n",
    "    for sentence in corpus:\n",
    "        for word in sentence.strip().split():\n",
    "            word_freqs[word] += 1\n",
    "    return word_freqs\n",
    "word_freqs = get_word_frequencies(corpus)\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411013c7-0a30-481c-bcc2-6d7ebce60bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab: [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# Remember: BPE starts from characters!\n",
    "def initialize_vocab_with_alphabet():\n",
    "    \"\"\"Initialize vocabulary with all lowercase English alphabet characters\"\"\"\n",
    "    # Include lowercase letters, digits, and common punctuation\n",
    "    vocab = set(string.ascii_lowercase + string.digits + string.punctuation + ' ')\n",
    "    return vocab\n",
    "initial_vocab = initialize_vocab_with_alphabet()\n",
    "print(f\"Initial vocab: {sorted(initial_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c42983f-724a-416a-a9d2-96a994c43fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How words are currently split:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': ['t', 'h', 'e'],\n",
       " 'cat': ['c', 'a', 't'],\n",
       " 'sat': ['s', 'a', 't'],\n",
       " 'on': ['o', 'n'],\n",
       " 'mat': ['m', 'a', 't'],\n",
       " 'dog': ['d', 'o', 'g'],\n",
       " 'log': ['l', 'o', 'g'],\n",
       " 'cats': ['c', 'a', 't', 's'],\n",
       " 'and': ['a', 'n', 'd'],\n",
       " 'dogs': ['d', 'o', 'g', 's'],\n",
       " 'are': ['a', 'r', 'e'],\n",
       " 'friends': ['f', 'r', 'i', 'e', 'n', 'd', 's']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All words in the initial corpus are initially tokenized as single characters\n",
    "splits = {word: list(word) for word in word_freqs.keys()}\n",
    "print(f\"How words are currently split:\")\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535bcaba-8304-44b0-ab47-03204206afa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent pair of character is ('a', 't') -> 5 occurrences\n"
     ]
    }
   ],
   "source": [
    "# To choose the most frequent pair we want to join, we must take into account of the word frequencies in the corpus\n",
    "def estimate_token_frequencies(word_freqs, current_splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    # For each word and associated frequency\n",
    "    for word, freq in word_freqs.items():\n",
    "        # Recover how that word is currently split into tokens\n",
    "        current_split = current_splits[word]\n",
    "        # Keep track of how frequent each combination of subsequent letter for that word is\n",
    "        for j in range(len(current_split) - 1):\n",
    "            pair = (current_split[j], current_split[j + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    # Find the most frequent pair\n",
    "    best_pair, occurrences = max(pair_freqs.items(), key=lambda x: x[1])\n",
    "    return best_pair, occurrences\n",
    "best_pair, occurrences = estimate_token_frequencies(word_freqs, splits)\n",
    "print(f\"The most frequent pair of character is {best_pair} -> {occurrences} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94009f3d-36c0-446e-8636-fece09b08cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocab: [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'at', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# Create new merged token and add to vocabulary\n",
    "new_token = ''.join(best_pair)\n",
    "vocab = {value for value in initial_vocab} # working copy of initial vocab\n",
    "vocab.add(new_token)\n",
    "print(f\"Updated vocab: {sorted(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605dca15-bff0-4401-9fc6-d47521cd15b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How words are split after the first merge:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': ['t', 'h', 'e'],\n",
       " 'cat': ['c', 'at'],\n",
       " 'sat': ['s', 'at'],\n",
       " 'on': ['o', 'n'],\n",
       " 'mat': ['m', 'at'],\n",
       " 'dog': ['d', 'o', 'g'],\n",
       " 'log': ['l', 'o', 'g'],\n",
       " 'cats': ['c', 'at', 's'],\n",
       " 'and': ['a', 'n', 'd'],\n",
       " 'dogs': ['d', 'o', 'g', 's'],\n",
       " 'are': ['a', 'r', 'e'],\n",
       " 'friends': ['f', 'r', 'i', 'e', 'n', 'd', 's']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, update the way each word is split into tokens according to the new merge!\n",
    "def update_word_splits(best_pair, word_freqs, splits):\n",
    "    for word in word_freqs.keys():\n",
    "        split = splits[word]\n",
    "        # Apply the merge to this word\n",
    "        new_split = []\n",
    "        i = 0\n",
    "        while i < len(split):\n",
    "            if i < len(split) - 1 and (split[i], split[i + 1]) == best_pair:\n",
    "                new_split.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_split.append(split[i])\n",
    "                i += 1\n",
    "        # Update the split for this word\n",
    "        splits[word] = new_split\n",
    "    return splits\n",
    "updated_splits = update_word_splits(best_pair, word_freqs, splits)\n",
    "print(f\"How words are split after the first merge:\")\n",
    "updated_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95dd9e63-af27-4583-815e-9052db1077a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer starting from corpus: ['the cat sat on the mat', 'the dog sat on the log', 'cats and dogs are friends']\n",
      "\tBase vocabulary size (unique characters): 69\n",
      "\tBase vocabulary: [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
      "\tMerge 1: ('a', 't') -> at (frequency: 5)\n",
      "\tMerge 2: ('t', 'h') -> th (frequency: 4)\n",
      "\tMerge 3: ('at', 'e') -> ate (frequency: 4)\n",
      "\tMerge 4: ('o', 'g') -> og (frequency: 3)\n",
      "\tMerge 5: ('c', 'at') -> cat (frequency: 2)\n",
      "\tMerge 6: ('s', 'at') -> sat (frequency: 2)\n",
      "\tMerge 7: ('o', 'n') -> on (frequency: 2)\n",
      "\tMerge 8: ('d', 'at') -> dat (frequency: 2)\n",
      "\tMerge 9: ('at', 's') -> ats (frequency: 2)\n",
      "\tMerge 10: ('n', 'd') -> nd (frequency: 2)\n",
      "\t\t- Base characters: 69\n",
      "\t\t- Merged tokens: 10\n",
      "\t\t- Total vocabulary size: 79\n",
      "\n",
      "Final vocabulary: [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'at', 'ate', 'ats', 'b', 'c', 'cat', 'd', 'dat', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'nd', 'o', 'og', 'on', 'p', 'q', 'r', 's', 'sat', 't', 'th', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
      "\n",
      "Final merges: {('a', 't'): 'at', ('t', 'h'): 'th', ('at', 'e'): 'ate', ('o', 'g'): 'og', ('c', 'at'): 'cat', ('s', 'at'): 'sat', ('o', 'n'): 'on', ('d', 'at'): 'dat', ('at', 's'): 'ats', ('n', 'd'): 'nd'}\n"
     ]
    }
   ],
   "source": [
    "# For BPE, simply repeat for num_merges!\n",
    "def train_bpe(corpus, num_merges):\n",
    "    \"\"\"Train a BPE tokenizer on the corpus\"\"\"\n",
    "    # Start by splitting sentences into words and count their frequencies\n",
    "    word_freqs = get_word_frequencies(corpus)\n",
    "    # Initialize the vocabulary with individual characters\n",
    "    vocab = initialize_vocab_with_alphabet()\n",
    "    print(f\"\\tBase vocabulary size (unique characters): {len(vocab)}\")\n",
    "    print(f\"\\tBase vocabulary: {sorted(vocab)}\")\n",
    "    # Initialize the list of merge operations\n",
    "    merges = {}\n",
    "    # Split words into characters \n",
    "    splits = {word: list(word) for word in word_freqs.keys()}\n",
    "    # Perform the specified number of merges\n",
    "    for i in range(num_merges):\n",
    "        # Select best pair, given current splits\n",
    "        best_pair, occurrences = estimate_token_frequencies(word_freqs, splits)\n",
    "        # Create new merged token and add to vocabulary\n",
    "        new_token = ''.join(best_pair)\n",
    "        vocab.add(new_token)\n",
    "        merges[best_pair] = new_token\n",
    "        print(f\"\\tMerge {i+1}: {best_pair} -> {new_token} (frequency: {occurrences})\")\n",
    "        splits = update_word_splits(best_pair, word_freqs, splits)    \n",
    "    # Calculate final vocabulary size\n",
    "    print(f\"\\t\\t- Base characters: {len(vocab) - len(merges)}\")\n",
    "    print(f\"\\t\\t- Merged tokens: {len(merges)}\")\n",
    "    print(f\"\\t\\t- Total vocabulary size: {len(vocab)}\")\n",
    "    # Return the vocabulary and merges for tokenization\n",
    "    return vocab, merges\n",
    "\n",
    "# Train BPE\n",
    "print(f\"Training BPE tokenizer starting from corpus: {corpus}\")\n",
    "vocab, merges = train_bpe(corpus, num_merges=10)\n",
    "print(f\"\\nFinal vocabulary: {sorted(vocab)}\")\n",
    "print(f\"\\nFinal merges: {merges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078397c2-4981-4af3-bcc1-2bf9d6f6245f",
   "metadata": {},
   "source": [
    "### Examples of tokenization using the trained algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68bd6261-188f-415f-bd6b-5862408c37b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text, merges):\n",
    "    \"\"\"Tokenize text using learned merges\"\"\"\n",
    "    # Start with character-level tokenization\n",
    "    tokens = list(text)\n",
    "    # Apply merges in order\n",
    "    for (first, second), merged in merges.items():\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if tokens[i] == first and tokens[i + 1] == second:\n",
    "                tokens[i] = merged\n",
    "                del tokens[i + 1]\n",
    "            else:\n",
    "                i += 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d1c6aa3-645c-438b-a4bb-bcd8cd72f2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Examples:\n",
      "'the' -> ['th', 'e']\n",
      "'cat' -> ['cat']\n",
      "'sat' -> ['sat']\n",
      "'cats' -> ['cat', 's']\n",
      "'friends' -> ['f', 'r', 'i', 'e', 'nd', 's']\n",
      "'catsat' -> ['cat', 'sat']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenization Examples:\")\n",
    "test_words = [\"the\", \"cat\", \"sat\", \"cats\", \"friends\", \"catsat\"]\n",
    "for word in test_words:\n",
    "    tokens = tokenize(word, merges)\n",
    "    print(f\"'{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af45d0dc-a891-49d6-a430-e71cb1cc9aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a full sentence:\n",
      "\n",
      "Full text tokenization:\n",
      "'the cat sat on the mat with friends' -> ['th', 'e', 'cat', 'sat', 'on', 'th', 'e', 'm', 'at', 'w', 'i', 'th', 'f', 'r', 'i', 'e', 'nd', 's']\n"
     ]
    }
   ],
   "source": [
    "print(\"With a full sentence:\")\n",
    "test_text = \"the cat sat on the mat with friends\"\n",
    "tokens = []\n",
    "for word in test_text.split():\n",
    "    tokens.extend(tokenize(word, merges))\n",
    "    \n",
    "print(f\"\\nFull text tokenization:\")\n",
    "print(f\"'{test_text}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71e8c2-f92d-4f51-b61f-3db3e7a5f1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
